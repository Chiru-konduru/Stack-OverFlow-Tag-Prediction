{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-25T13:52:17.392108Z","iopub.execute_input":"2022-08-25T13:52:17.392634Z","iopub.status.idle":"2022-08-25T13:52:17.439781Z","shell.execute_reply.started":"2022-08-25T13:52:17.392523Z","shell.execute_reply":"2022-08-25T13:52:17.438776Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport warnings\nimport datetime as datetime\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:52:17.441788Z","iopub.execute_input":"2022-08-25T13:52:17.442457Z","iopub.status.idle":"2022-08-25T13:52:19.676636Z","shell.execute_reply.started":"2022-08-25T13:52:17.442419Z","shell.execute_reply":"2022-08-25T13:52:19.675491Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/facebook-recruiting-iii-keyword-extraction/Train/Train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:52:19.678227Z","iopub.execute_input":"2022-08-25T13:52:19.679246Z","iopub.status.idle":"2022-08-25T13:55:12.154992Z","shell.execute_reply.started":"2022-08-25T13:52:19.679205Z","shell.execute_reply":"2022-08-25T13:55:12.153607Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Performing EDA","metadata":{}},{"cell_type":"code","source":"df.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.158188Z","iopub.execute_input":"2022-08-25T13:55:12.159498Z","iopub.status.idle":"2022-08-25T13:55:12.169280Z","shell.execute_reply.started":"2022-08-25T13:55:12.159451Z","shell.execute_reply":"2022-08-25T13:55:12.167449Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# As this is large dataset I'll consider a small subset of it (50k rows) to train.\n#Slicing the dataset.\ndf = df.iloc[:50000, :]\ndf.shape\n\n#data['Tags'].value_counts() = 38783","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.170986Z","iopub.execute_input":"2022-08-25T13:55:12.172158Z","iopub.status.idle":"2022-08-25T13:55:12.187429Z","shell.execute_reply.started":"2022-08-25T13:55:12.172104Z","shell.execute_reply":"2022-08-25T13:55:12.185537Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Checking for duplicates\nduplicates = df.sort_values('Title', ascending=False).duplicated('Title')\nprint(\"Total number of duplicate questions : \", duplicates.sum())\ndf = df[~duplicates]\nprint(\"Dataframe shape without duplicates: \", df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.188826Z","iopub.execute_input":"2022-08-25T13:55:12.189241Z","iopub.status.idle":"2022-08-25T13:55:12.323945Z","shell.execute_reply.started":"2022-08-25T13:55:12.189158Z","shell.execute_reply":"2022-08-25T13:55:12.322832Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Counting the numer of tags per row(question) in training data. Split() helps in separating the \n#string into list and we are counting the length of each list.\ndef count_tags(x):\n    return len(x.split())\ndf[\"tag_count\"] = df[\"Tags\"].apply(count_tags)\nprint(\"tag count:\", df.tag_count)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.325434Z","iopub.execute_input":"2022-08-25T13:55:12.325858Z","iopub.status.idle":"2022-08-25T13:55:12.375638Z","shell.execute_reply.started":"2022-08-25T13:55:12.325821Z","shell.execute_reply":"2022-08-25T13:55:12.374013Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Frequency of tags \nprint(df['tag_count'].value_counts())\n#Average no of tags per question\nprint(df['tag_count'].mean())\n\n# we can observe on avg there are three tags per question.","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.377958Z","iopub.execute_input":"2022-08-25T13:55:12.378685Z","iopub.status.idle":"2022-08-25T13:55:12.391273Z","shell.execute_reply.started":"2022-08-25T13:55:12.378617Z","shell.execute_reply":"2022-08-25T13:55:12.389316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# dropping columns with na\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.393108Z","iopub.execute_input":"2022-08-25T13:55:12.393674Z","iopub.status.idle":"2022-08-25T13:55:12.423882Z","shell.execute_reply.started":"2022-08-25T13:55:12.393622Z","shell.execute_reply":"2022-08-25T13:55:12.422641Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#vectorizer = CountVectorizer(tokenizer= lambda text : text.split(\" \"))\n#tag_dtm = vectorizer.fit_transform(df[\"Tags\"])","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.429732Z","iopub.execute_input":"2022-08-25T13:55:12.430137Z","iopub.status.idle":"2022-08-25T13:55:12.436549Z","shell.execute_reply.started":"2022-08-25T13:55:12.430099Z","shell.execute_reply":"2022-08-25T13:55:12.434566Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Term Frequency-Inverse Document Frequency model (TFIDF)\ntv=TfidfVectorizer(tokenizer= lambda text : text.split(\" \"))\ntag_dtm=tv.fit_transform(df[\"Tags\"])","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.438740Z","iopub.execute_input":"2022-08-25T13:55:12.439151Z","iopub.status.idle":"2022-08-25T13:55:12.750828Z","shell.execute_reply.started":"2022-08-25T13:55:12.439116Z","shell.execute_reply":"2022-08-25T13:55:12.749305Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tags = tv.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.752396Z","iopub.execute_input":"2022-08-25T13:55:12.752874Z","iopub.status.idle":"2022-08-25T13:55:12.770019Z","shell.execute_reply.started":"2022-08-25T13:55:12.752830Z","shell.execute_reply":"2022-08-25T13:55:12.768568Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"freqs = tag_dtm.sum(axis=0).A1\nresult = dict(zip(tags,freqs))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.771843Z","iopub.execute_input":"2022-08-25T13:55:12.772334Z","iopub.status.idle":"2022-08-25T13:55:12.788926Z","shell.execute_reply.started":"2022-08-25T13:55:12.772287Z","shell.execute_reply":"2022-08-25T13:55:12.787700Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tag_df = pd.DataFrame(result.items(), columns=[\"Tags\", \"Counts\"])\ntag_df","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.790479Z","iopub.execute_input":"2022-08-25T13:55:12.790885Z","iopub.status.idle":"2022-08-25T13:55:12.817870Z","shell.execute_reply.started":"2022-08-25T13:55:12.790851Z","shell.execute_reply":"2022-08-25T13:55:12.816646Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\ntag_counts = tag_df_sorted[\"Counts\"].values","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.819693Z","iopub.execute_input":"2022-08-25T13:55:12.820074Z","iopub.status.idle":"2022-08-25T13:55:12.835001Z","shell.execute_reply.started":"2022-08-25T13:55:12.820030Z","shell.execute_reply":"2022-08-25T13:55:12.833901Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(background_color='black',\n         width = 1400,\n         height = 800).generate_from_frequencies(result)\nplt.figure(figsize=(30,20))\nplt.imshow(wordcloud)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:12.836688Z","iopub.execute_input":"2022-08-25T13:55:12.837475Z","iopub.status.idle":"2022-08-25T13:55:16.444561Z","shell.execute_reply.started":"2022-08-25T13:55:12.837424Z","shell.execute_reply":"2022-08-25T13:55:16.443363Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"i = np.arange(30)\ntag_df_sorted.head(30).plot(kind='bar')\nplt.xticks(i, tag_df_sorted['Tags'][:30])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:16.446067Z","iopub.execute_input":"2022-08-25T13:55:16.446439Z","iopub.status.idle":"2022-08-25T13:55:16.837558Z","shell.execute_reply.started":"2022-08-25T13:55:16.446405Z","shell.execute_reply":"2022-08-25T13:55:16.836376Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Stripping HTML and stop words\ndef striphtml(data):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr,' ',str(data))\n    return cleantext\n\nstop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer('english')","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:16.839257Z","iopub.execute_input":"2022-08-25T13:55:16.840574Z","iopub.status.idle":"2022-08-25T13:55:16.854312Z","shell.execute_reply.started":"2022-08-25T13:55:16.840500Z","shell.execute_reply":"2022-08-25T13:55:16.852664Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Text Preprocessing**","metadata":{}},{"cell_type":"code","source":"question_list=[]\nquestions_with_code = 0\nlength_preprocessing = 0 \nlength_postprocessing = 0 \nfor index,row in df.iterrows():\n    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"Tags\"]\n    if '<code>' in body:\n        questions_with_code+=1\n    length_preprocessing+=len(title) + len(body)\n    body=re.sub('<code>(.*?)</code>', '', body, flags=re.MULTILINE|re.DOTALL)\n    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n    title=title.encode('utf-8')\n    question=str(title)+\" \"+str(body)\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n    question_list.append(question)\n    length_postprocessing += len(question)\ndf[\"question\"] = question_list\navg_len_before_preprocessing=(length_preprocessing*1.0)/df.shape[0]\navg_len_after_preprocessing=(length_postprocessing*1.0)/df.shape[0]\n\nprint( \"Avg. length of questions(Title+Body) before Text preprocessing: \", avg_len_before_preprocessing)\nprint( \"Avg. length of questions(Title+Body) post Text processing: \", avg_len_after_preprocessing)\nprint (\"% of questions containing code: \", (questions_with_code*100.0)/df.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:55:16.856544Z","iopub.execute_input":"2022-08-25T13:55:16.856926Z","iopub.status.idle":"2022-08-25T13:56:38.621539Z","shell.execute_reply.started":"2022-08-25T13:55:16.856888Z","shell.execute_reply":"2022-08-25T13:56:38.619986Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"preprocessed_df = df[[\"question\",\"Tags\"]]\nprint(\"Shape of preprocessed data :\", preprocessed_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:38.623546Z","iopub.execute_input":"2022-08-25T13:56:38.623973Z","iopub.status.idle":"2022-08-25T13:56:38.645858Z","shell.execute_reply.started":"2022-08-25T13:56:38.623937Z","shell.execute_reply":"2022-08-25T13:56:38.644483Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**ML Modelling**","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n#vectorizer=TfidfVectorizer(tokenizer= lambda text : text.split(\" \"), binary='true')\ny_multilabel = vectorizer.fit_transform(preprocessed_df['Tags'])","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:38.647303Z","iopub.execute_input":"2022-08-25T13:56:38.647683Z","iopub.status.idle":"2022-08-25T13:56:38.936928Z","shell.execute_reply.started":"2022-08-25T13:56:38.647648Z","shell.execute_reply":"2022-08-25T13:56:38.935612Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"y_multilabel.get_shape()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:38.938483Z","iopub.execute_input":"2022-08-25T13:56:38.938900Z","iopub.status.idle":"2022-08-25T13:56:38.945418Z","shell.execute_reply.started":"2022-08-25T13:56:38.938865Z","shell.execute_reply":"2022-08-25T13:56:38.944486Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def tags_to_select(n):\n    sum_of_tags = y_multilabel.sum(axis=0).tolist()[0]\n    sorted_tags = sorted(range(len(sum_of_tags)), key=lambda i: sum_of_tags[i], reverse=True)\n    ny_multilabel=y_multilabel[:,sorted_tags[:n]]\n    return ny_multilabel\n\ndef questions_considered_fn(n):\n    ny_multilabel = tags_to_select(n)\n    x= ny_multilabel.sum(axis=1)\n    return (np.count_nonzero(x==0))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:38.947078Z","iopub.execute_input":"2022-08-25T13:56:38.947427Z","iopub.status.idle":"2022-08-25T13:56:38.961975Z","shell.execute_reply.started":"2022-08-25T13:56:38.947396Z","shell.execute_reply":"2022-08-25T13:56:38.960655Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"question_explained = []\ntotal_tags = y_multilabel.shape[1]\ntotal_qs = preprocessed_df.shape[0]\n\nfor i in range(1000, total_tags, 100):\n    question_explained.append(np.round(((total_qs-questions_considered_fn(i))/total_qs)*100,3))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:38.963089Z","iopub.execute_input":"2022-08-25T13:56:38.963494Z","iopub.status.idle":"2022-08-25T13:56:40.377085Z","shell.execute_reply.started":"2022-08-25T13:56:38.963459Z","shell.execute_reply":"2022-08-25T13:56:40.375993Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(question_explained)\nxlabel = list(500+np.array(range(-50,450,50))*50)\nax.set_xticklabels(xlabel)\nplt.xlabel(\"Number of tags\")\nplt.ylabel(\"Number Questions coverd partially\")\nplt.grid()\nplt.show()\n# you can choose any number of tags based on your computing power, minimun is 50(it covers 90% of the tags)\nprint(\"with \",1000,\"tags we are covering \",question_explained[50],\"% of questions\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:40.378877Z","iopub.execute_input":"2022-08-25T13:56:40.379344Z","iopub.status.idle":"2022-08-25T13:56:40.559997Z","shell.execute_reply.started":"2022-08-25T13:56:40.379311Z","shell.execute_reply":"2022-08-25T13:56:40.558674Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"yx_multilabel = tags_to_select(1000)\nprint(\"number of questions that are not covered :\", questions_considered_fn(12500),\"out of \", total_qs)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:40.561612Z","iopub.execute_input":"2022-08-25T13:56:40.562384Z","iopub.status.idle":"2022-08-25T13:56:40.590670Z","shell.execute_reply.started":"2022-08-25T13:56:40.562044Z","shell.execute_reply":"2022-08-25T13:56:40.589112Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"yx_multilabel.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:40.592284Z","iopub.execute_input":"2022-08-25T13:56:40.592782Z","iopub.status.idle":"2022-08-25T13:56:40.601183Z","shell.execute_reply.started":"2022-08-25T13:56:40.592735Z","shell.execute_reply":"2022-08-25T13:56:40.599890Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#splitting data\ntotal_size=preprocessed_df.shape[0]\ntrain_size=int(0.80*total_size)\n\nx_train=preprocessed_df.head(train_size)\nx_test=preprocessed_df.tail(total_size - train_size)\n\ny_train = yx_multilabel[0:train_size,:]\ny_test = yx_multilabel[train_size:total_size,:]","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:40.609231Z","iopub.execute_input":"2022-08-25T13:56:40.609682Z","iopub.status.idle":"2022-08-25T13:56:40.621734Z","shell.execute_reply.started":"2022-08-25T13:56:40.609643Z","shell.execute_reply":"2022-08-25T13:56:40.619791Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"print(\"Number of data points in train data :\", y_train.shape)\nprint(\"Number of data points in test data :\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:40.623865Z","iopub.execute_input":"2022-08-25T13:56:40.624761Z","iopub.status.idle":"2022-08-25T13:56:40.634309Z","shell.execute_reply.started":"2022-08-25T13:56:40.624703Z","shell.execute_reply":"2022-08-25T13:56:40.633130Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#Featurizing the data\ntfidf_vect = TfidfVectorizer(min_df=0.00009,max_features=200000,smooth_idf=True,norm='l2',\\\n               tokenizer=lambda x : x.split(),sublinear_tf=False, ngram_range=(1,3) )","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:40.635790Z","iopub.execute_input":"2022-08-25T13:56:40.636126Z","iopub.status.idle":"2022-08-25T13:56:40.650764Z","shell.execute_reply.started":"2022-08-25T13:56:40.636096Z","shell.execute_reply":"2022-08-25T13:56:40.649236Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"x_train_vectors = tfidf_vect.fit_transform(x_train['question'])\nx_test_vectors = tfidf_vect.transform(x_test['question'])","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:56:40.652493Z","iopub.execute_input":"2022-08-25T13:56:40.652901Z","iopub.status.idle":"2022-08-25T13:57:03.995720Z","shell.execute_reply.started":"2022-08-25T13:56:40.652856Z","shell.execute_reply":"2022-08-25T13:57:03.994308Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Applying Logistic Regression with OneVsRest Classifier\nclassifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\nclassifier.fit(x_train_vectors,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:57:03.997368Z","iopub.execute_input":"2022-08-25T13:57:03.998133Z","iopub.status.idle":"2022-08-25T13:59:33.633417Z","shell.execute_reply.started":"2022-08-25T13:57:03.998092Z","shell.execute_reply":"2022-08-25T13:59:33.631745Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"predictions = classifier.predict(x_test_vectors)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:59:33.635423Z","iopub.execute_input":"2022-08-25T13:59:33.635831Z","iopub.status.idle":"2022-08-25T13:59:35.818576Z","shell.execute_reply.started":"2022-08-25T13:59:33.635794Z","shell.execute_reply":"2022-08-25T13:59:35.817486Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(\"accuracy \", metrics.accuracy_score(y_test,predictions))\nprint(\"macro f1 score \",metrics.f1_score(y_test,predictions, average='macro'))\nprint(\"micro f1 score \", metrics.f1_score(y_test, predictions, average='micro'))\nprint(\"hamming loss \", metrics.hamming_loss(y_test,predictions))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T13:59:35.820198Z","iopub.execute_input":"2022-08-25T13:59:35.820684Z","iopub.status.idle":"2022-08-25T13:59:35.846087Z","shell.execute_reply.started":"2022-08-25T13:59:35.820631Z","shell.execute_reply":"2022-08-25T13:59:35.844588Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"**OneVsRest Classifier with Linear SVM (Loss-Hinge)**","metadata":{}},{"cell_type":"code","source":"#start = datetime.now()\nclassifier_5 = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'), n_jobs=-1)\nclassifier_5.fit(x_train_vectors, y_train)\n#Saving the Classifier\n#joblib.dump(classifier_5, 'lr_with_more_title_weight_5.pkl') \n\n#Loading the Classifier\n#classifier_5 = joblib.load('../input/d/elemento/facebook-recruiting-iii-keyword-extraction/lr_with_more_title_weight_5.pkl') \npredictions = classifier_5.predict(x_test_vectors)\n\nprint(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\nprint(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n\nprecision = precision_score(y_test, predictions, average='micro')\nrecall = recall_score(y_test, predictions, average='micro')\nf1 = f1_score(y_test, predictions, average='micro')\n \nprint(\"Micro-average quality numbers\")\nprint(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n\nprecision = precision_score(y_test, predictions, average='macro')\nrecall = recall_score(y_test, predictions, average='macro')\nf1 = f1_score(y_test, predictions, average='macro')\n \nprint(\"Macro-average quality numbers\")\nprint(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n\nprint (metrics.classification_report(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T14:25:13.894307Z","iopub.execute_input":"2022-08-25T14:25:13.895910Z","iopub.status.idle":"2022-08-25T14:26:59.109549Z","shell.execute_reply.started":"2022-08-25T14:25:13.895859Z","shell.execute_reply":"2022-08-25T14:26:59.106759Z"},"trusted":true},"execution_count":40,"outputs":[]}]}