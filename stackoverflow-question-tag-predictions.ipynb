{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-02T02:25:39.522705Z","iopub.execute_input":"2022-07-02T02:25:39.523246Z","iopub.status.idle":"2022-07-02T02:25:39.553521Z","shell.execute_reply.started":"2022-07-02T02:25:39.523138Z","shell.execute_reply":"2022-07-02T02:25:39.551972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport warnings\nimport datetime as datetime\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:25:39.556467Z","iopub.execute_input":"2022-07-02T02:25:39.556952Z","iopub.status.idle":"2022-07-02T02:25:41.356717Z","shell.execute_reply.started":"2022-07-02T02:25:39.556901Z","shell.execute_reply":"2022-07-02T02:25:41.355783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/facebook-recruiting-iii-keyword-extraction/Train/Train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:25:41.359096Z","iopub.execute_input":"2022-07-02T02:25:41.359855Z","iopub.status.idle":"2022-07-02T02:28:39.399496Z","shell.execute_reply.started":"2022-07-02T02:25:41.359804Z","shell.execute_reply":"2022-07-02T02:28:39.39859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performing EDA","metadata":{}},{"cell_type":"code","source":"df.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:39.400975Z","iopub.execute_input":"2022-07-02T02:28:39.401573Z","iopub.status.idle":"2022-07-02T02:28:39.408586Z","shell.execute_reply.started":"2022-07-02T02:28:39.40154Z","shell.execute_reply":"2022-07-02T02:28:39.407386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As this is large dataset I'll consider a small subset of it (50k rows) to train.\n#Slicing the dataset.\ndf = df.iloc[:50000, :]\ndf.shape\n\n#data['Tags'].value_counts() = 38783","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:39.411865Z","iopub.execute_input":"2022-07-02T02:28:39.412319Z","iopub.status.idle":"2022-07-02T02:28:39.422323Z","shell.execute_reply.started":"2022-07-02T02:28:39.412282Z","shell.execute_reply":"2022-07-02T02:28:39.421292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking for duplicates\nduplicates = df.sort_values('Title', ascending=False).duplicated('Title')\nprint(\"Total number of duplicate questions : \", duplicates.sum())\ndf = df[~duplicates]\nprint(\"Dataframe shape without duplicates: \", df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:39.424442Z","iopub.execute_input":"2022-07-02T02:28:39.424944Z","iopub.status.idle":"2022-07-02T02:28:39.579087Z","shell.execute_reply.started":"2022-07-02T02:28:39.424904Z","shell.execute_reply":"2022-07-02T02:28:39.577879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counting the numer of tags per row(question) in training data. Split() helps in separating the \n#string into list and we are counting the length of each list.\ndef count_tags(x):\n    return len(x.split())\ndf[\"tag_count\"] = df[\"Tags\"].apply(count_tags)\nprint(\"tag count:\", df.tag_count)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:39.580527Z","iopub.execute_input":"2022-07-02T02:28:39.580893Z","iopub.status.idle":"2022-07-02T02:28:39.639346Z","shell.execute_reply.started":"2022-07-02T02:28:39.580862Z","shell.execute_reply":"2022-07-02T02:28:39.638084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Frequency of tags \nprint(df['tag_count'].value_counts())\n#Average no of tags per question\nprint(df['tag_count'].mean())\n\n# we can observe on avg there are three tags per question.","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:39.641326Z","iopub.execute_input":"2022-07-02T02:28:39.641696Z","iopub.status.idle":"2022-07-02T02:28:39.651958Z","shell.execute_reply.started":"2022-07-02T02:28:39.641663Z","shell.execute_reply":"2022-07-02T02:28:39.650719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping columns with na\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:39.653573Z","iopub.execute_input":"2022-07-02T02:28:39.654492Z","iopub.status.idle":"2022-07-02T02:28:39.693651Z","shell.execute_reply.started":"2022-07-02T02:28:39.65445Z","shell.execute_reply":"2022-07-02T02:28:39.69246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vectorizer = CountVectorizer(tokenizer= lambda text : text.split(\" \"))\n#tag_dtm = vectorizer.fit_transform(df[\"Tags\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:39.695315Z","iopub.execute_input":"2022-07-02T02:28:39.695703Z","iopub.status.idle":"2022-07-02T02:28:39.700404Z","shell.execute_reply.started":"2022-07-02T02:28:39.695667Z","shell.execute_reply":"2022-07-02T02:28:39.699122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Term Frequency-Inverse Document Frequency model (TFIDF)\ntv=TfidfVectorizer(tokenizer= lambda text : text.split(\" \"))\ntag_dtm=tv.fit_transform(df[\"Tags\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:39.704565Z","iopub.execute_input":"2022-07-02T02:28:39.705052Z","iopub.status.idle":"2022-07-02T02:28:40.051727Z","shell.execute_reply.started":"2022-07-02T02:28:39.704997Z","shell.execute_reply":"2022-07-02T02:28:40.050163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags = tv.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:40.053243Z","iopub.execute_input":"2022-07-02T02:28:40.053629Z","iopub.status.idle":"2022-07-02T02:28:40.0695Z","shell.execute_reply.started":"2022-07-02T02:28:40.053593Z","shell.execute_reply":"2022-07-02T02:28:40.068125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freqs = tag_dtm.sum(axis=0).A1\nresult = dict(zip(tags,freqs))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:40.071552Z","iopub.execute_input":"2022-07-02T02:28:40.072449Z","iopub.status.idle":"2022-07-02T02:28:40.087275Z","shell.execute_reply.started":"2022-07-02T02:28:40.072386Z","shell.execute_reply":"2022-07-02T02:28:40.08587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_df = pd.DataFrame(result.items(), columns=[\"Tags\", \"Counts\"])\ntag_df","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:40.090353Z","iopub.execute_input":"2022-07-02T02:28:40.090901Z","iopub.status.idle":"2022-07-02T02:28:40.121071Z","shell.execute_reply.started":"2022-07-02T02:28:40.09085Z","shell.execute_reply":"2022-07-02T02:28:40.119825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\ntag_counts = tag_df_sorted[\"Counts\"].values","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:40.123896Z","iopub.execute_input":"2022-07-02T02:28:40.124305Z","iopub.status.idle":"2022-07-02T02:28:40.132706Z","shell.execute_reply.started":"2022-07-02T02:28:40.12427Z","shell.execute_reply":"2022-07-02T02:28:40.131592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(background_color='black',\n         width = 1400,\n         height = 800).generate_from_frequencies(result)\nplt.figure(figsize=(30,20))\nplt.imshow(wordcloud)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:40.134707Z","iopub.execute_input":"2022-07-02T02:28:40.135524Z","iopub.status.idle":"2022-07-02T02:28:43.483144Z","shell.execute_reply.started":"2022-07-02T02:28:40.135471Z","shell.execute_reply":"2022-07-02T02:28:43.481741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = np.arange(30)\ntag_df_sorted.head(30).plot(kind='bar')\nplt.xticks(i, tag_df_sorted['Tags'][:30])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:43.48472Z","iopub.execute_input":"2022-07-02T02:28:43.485202Z","iopub.status.idle":"2022-07-02T02:28:43.827772Z","shell.execute_reply.started":"2022-07-02T02:28:43.485157Z","shell.execute_reply":"2022-07-02T02:28:43.826419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stripping HTML and stop words\ndef striphtml(data):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr,' ',str(data))\n    return cleantext\n\nstop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer('english')","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:43.82969Z","iopub.execute_input":"2022-07-02T02:28:43.830225Z","iopub.status.idle":"2022-07-02T02:28:43.842701Z","shell.execute_reply.started":"2022-07-02T02:28:43.830174Z","shell.execute_reply":"2022-07-02T02:28:43.841479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Text Preprocessing**","metadata":{}},{"cell_type":"code","source":"question_list=[]\nquestions_with_code = 0\nlength_preprocessing = 0 \nlength_postprocessing = 0 \nfor index,row in df.iterrows():\n    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"Tags\"]\n    if '<code>' in body:\n        questions_with_code+=1\n    length_preprocessing+=len(title) + len(body)\n    body=re.sub('<code>(.*?)</code>', '', body, flags=re.MULTILINE|re.DOTALL)\n    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n    title=title.encode('utf-8')\n    question=str(title)+\" \"+str(body)\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n    question_list.append(question)\n    length_postprocessing += len(question)\ndf[\"question\"] = question_list\navg_len_before_preprocessing=(length_preprocessing*1.0)/df.shape[0]\navg_len_after_preprocessing=(length_postprocessing*1.0)/df.shape[0]\n\nprint( \"Avg. length of questions(Title+Body) before Text preprocessing: \", avg_len_before_preprocessing)\nprint( \"Avg. length of questions(Title+Body) post Text processing: \", avg_len_after_preprocessing)\nprint (\"% of questions containing code: \", (questions_with_code*100.0)/df.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:28:43.844483Z","iopub.execute_input":"2022-07-02T02:28:43.844857Z","iopub.status.idle":"2022-07-02T02:30:35.839487Z","shell.execute_reply.started":"2022-07-02T02:28:43.844826Z","shell.execute_reply":"2022-07-02T02:30:35.838151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessed_df = df[[\"question\",\"Tags\"]]\nprint(\"Shape of preprocessed data :\", preprocessed_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:35.841289Z","iopub.execute_input":"2022-07-02T02:30:35.841629Z","iopub.status.idle":"2022-07-02T02:30:35.8671Z","shell.execute_reply.started":"2022-07-02T02:30:35.841598Z","shell.execute_reply":"2022-07-02T02:30:35.865466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ML Modelling**","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n#vectorizer=TfidfVectorizer(tokenizer= lambda text : text.split(\" \"), binary='true')\ny_multilabel = vectorizer.fit_transform(preprocessed_df['Tags'])","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:35.869053Z","iopub.execute_input":"2022-07-02T02:30:35.869434Z","iopub.status.idle":"2022-07-02T02:30:36.19811Z","shell.execute_reply.started":"2022-07-02T02:30:35.869399Z","shell.execute_reply":"2022-07-02T02:30:36.196817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_multilabel.get_shape()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:36.199516Z","iopub.execute_input":"2022-07-02T02:30:36.199859Z","iopub.status.idle":"2022-07-02T02:30:36.207882Z","shell.execute_reply.started":"2022-07-02T02:30:36.199828Z","shell.execute_reply":"2022-07-02T02:30:36.206593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tags_to_select(n):\n    sum_of_tags = y_multilabel.sum(axis=0).tolist()[0]\n    sorted_tags = sorted(range(len(sum_of_tags)), key=lambda i: sum_of_tags[i], reverse=True)\n    ny_multilabel=y_multilabel[:,sorted_tags[:n]]\n    return ny_multilabel\n\ndef questions_considered_fn(n):\n    ny_multilabel = tags_to_select(n)\n    x= ny_multilabel.sum(axis=1)\n    return (np.count_nonzero(x==0))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:36.209833Z","iopub.execute_input":"2022-07-02T02:30:36.210237Z","iopub.status.idle":"2022-07-02T02:30:36.220186Z","shell.execute_reply.started":"2022-07-02T02:30:36.210191Z","shell.execute_reply":"2022-07-02T02:30:36.218868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_explained = []\ntotal_tags = y_multilabel.shape[1]\ntotal_qs = preprocessed_df.shape[0]\n\nfor i in range(1000, total_tags, 100):\n    question_explained.append(np.round(((total_qs-questions_considered_fn(i))/total_qs)*100,3))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:36.222369Z","iopub.execute_input":"2022-07-02T02:30:36.222927Z","iopub.status.idle":"2022-07-02T02:30:37.62391Z","shell.execute_reply.started":"2022-07-02T02:30:36.22289Z","shell.execute_reply":"2022-07-02T02:30:37.622562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(question_explained)\nxlabel = list(500+np.array(range(-50,450,50))*50)\nax.set_xticklabels(xlabel)\nplt.xlabel(\"Number of tags\")\nplt.ylabel(\"Number Questions coverd partially\")\nplt.grid()\nplt.show()\n# you can choose any number of tags based on your computing power, minimun is 50(it covers 90% of the tags)\nprint(\"with \",1000,\"tags we are covering \",question_explained[50],\"% of questions\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:37.625348Z","iopub.execute_input":"2022-07-02T02:30:37.625725Z","iopub.status.idle":"2022-07-02T02:30:37.799788Z","shell.execute_reply.started":"2022-07-02T02:30:37.62569Z","shell.execute_reply":"2022-07-02T02:30:37.798791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yx_multilabel = tags_to_select(1000)\nprint(\"number of questions that are not covered :\", questions_considered_fn(12500),\"out of \", total_qs)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:37.801114Z","iopub.execute_input":"2022-07-02T02:30:37.801648Z","iopub.status.idle":"2022-07-02T02:30:37.828428Z","shell.execute_reply.started":"2022-07-02T02:30:37.801614Z","shell.execute_reply":"2022-07-02T02:30:37.827099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yx_multilabel.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:37.830098Z","iopub.execute_input":"2022-07-02T02:30:37.830454Z","iopub.status.idle":"2022-07-02T02:30:37.837226Z","shell.execute_reply.started":"2022-07-02T02:30:37.830423Z","shell.execute_reply":"2022-07-02T02:30:37.835976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting data\ntotal_size=preprocessed_df.shape[0]\ntrain_size=int(0.80*total_size)\n\nx_train=preprocessed_df.head(train_size)\nx_test=preprocessed_df.tail(total_size - train_size)\n\ny_train = yx_multilabel[0:train_size,:]\ny_test = yx_multilabel[train_size:total_size,:]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:37.838746Z","iopub.execute_input":"2022-07-02T02:30:37.839574Z","iopub.status.idle":"2022-07-02T02:30:37.852052Z","shell.execute_reply.started":"2022-07-02T02:30:37.839535Z","shell.execute_reply":"2022-07-02T02:30:37.850429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of data points in train data :\", y_train.shape)\nprint(\"Number of data points in test data :\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:37.859898Z","iopub.execute_input":"2022-07-02T02:30:37.861436Z","iopub.status.idle":"2022-07-02T02:30:37.868671Z","shell.execute_reply.started":"2022-07-02T02:30:37.861386Z","shell.execute_reply":"2022-07-02T02:30:37.867078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Featurizing the data\ntfidf_vect = TfidfVectorizer(min_df=0.00009,max_features=200000,smooth_idf=True,norm='l2',\\\n               tokenizer=lambda x : x.split(),sublinear_tf=False, ngram_range=(1,3) )","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:37.870238Z","iopub.execute_input":"2022-07-02T02:30:37.870593Z","iopub.status.idle":"2022-07-02T02:30:37.881943Z","shell.execute_reply.started":"2022-07-02T02:30:37.870559Z","shell.execute_reply":"2022-07-02T02:30:37.880866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_vectors = tfidf_vect.fit_transform(x_train['question'])\nx_test_vectors = tfidf_vect.transform(x_test['question'])","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:30:37.884438Z","iopub.execute_input":"2022-07-02T02:30:37.885385Z","iopub.status.idle":"2022-07-02T02:31:03.752657Z","shell.execute_reply.started":"2022-07-02T02:30:37.885336Z","shell.execute_reply":"2022-07-02T02:31:03.751544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying Logistic Regression with OneVsRest Classifier\nclassifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\nclassifier.fit(x_train_vectors,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:31:03.754752Z","iopub.execute_input":"2022-07-02T02:31:03.75565Z","iopub.status.idle":"2022-07-02T02:33:32.868467Z","shell.execute_reply.started":"2022-07-02T02:31:03.755599Z","shell.execute_reply":"2022-07-02T02:33:32.867116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = classifier.predict(x_test_vectors)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:33:32.870467Z","iopub.execute_input":"2022-07-02T02:33:32.870826Z","iopub.status.idle":"2022-07-02T02:33:35.102953Z","shell.execute_reply.started":"2022-07-02T02:33:32.870789Z","shell.execute_reply":"2022-07-02T02:33:35.101377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"accuracy \", metrics.accuracy_score(y_test,predictions))\nprint(\"macro f1 score \",metrics.f1_score(y_test,predictions, average='macro'))\nprint(\"micro f1 score \", metrics.f1_score(y_test, predictions, average='micro'))\nprint(\"hamming loss \", metrics.hamming_loss(y_test,predictions))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T02:33:35.104394Z","iopub.execute_input":"2022-07-02T02:33:35.104736Z","iopub.status.idle":"2022-07-02T02:33:35.132208Z","shell.execute_reply.started":"2022-07-02T02:33:35.104707Z","shell.execute_reply":"2022-07-02T02:33:35.130895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OneVsRest Classifier with Linear SVM (Loss-Hinge)**","metadata":{}},{"cell_type":"code","source":"start = datetime.now()\nclassifier_5 = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'), n_jobs=-1)\nclassifier_5.fit(x_train_multilabel, y_train)\nSaving the Classifier\njoblib.dump(classifier_5, 'lr_with_more_title_weight_5.pkl') \n\nLoading the Classifier\nclassifier_5 = joblib.load('../input/d/elemento/facebook-recruiting-iii-keyword-extraction/lr_with_more_title_weight_5.pkl') \npredictions = classifier_5.predict(x_test_multilabel)\n\nprint(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\nprint(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n\nprecision = precision_score(y_test, predictions, average='micro')\nrecall = recall_score(y_test, predictions, average='micro')\nf1 = f1_score(y_test, predictions, average='micro')\n \nprint(\"Micro-average quality numbers\")\nprint(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n\nprecision = precision_score(y_test, predictions, average='macro')\nrecall = recall_score(y_test, predictions, average='macro')\nf1 = f1_score(y_test, predictions, average='macro')\n \nprint(\"Macro-average quality numbers\")\nprint(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n\nprint (metrics.classification_report(y_test, predictions))\nprint(\"Time taken to run this cell :\", datetime.now() - start)","metadata":{},"execution_count":null,"outputs":[]}]}